{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "os.chdir('..')\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from model import MemoryCell\n",
    "from train import calculate_accuracy\n",
    "\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get token-lvl accuracy (all tokens except the first one)\n",
    "# get ppl on non-prefix text (all tokens except the first one)\n",
    "# args:\n",
    "# - model name (model)\n",
    "# - prefix (left context) length\n",
    "# - suffix length (compressed text) length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EleutherAI/pythia-410m'\n",
    "dtype = 'float32'\n",
    "device = 'cuda'\n",
    "use_flash_attention_2 = False\n",
    "\n",
    "# model_name = 'meta-llama/Llama-3.2-1B'\n",
    "# dtype = 'bfloat16'\n",
    "# device = 'cuda'\n",
    "# use_flash_attention_2 = True\n",
    "\n",
    "dtype = getattr(torch, dtype)\n",
    "N_mem_tokens = 1\n",
    "max_length = 64\n",
    "prefix_length = 2\n",
    "texts_path = './data/pg19_valid_1k_chunks.csv'\n",
    "mem_results_path = Path(f'./runs/{model_name}/mem_{N_mem_tokens}_len_{max_length}.pkl')\n",
    "\n",
    "with_prefix_results_path = mem_results_path.parent / 'with_prefix' / f'mem_{N_mem_tokens}_len_{max_length}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/py3.11_pt2_cu11.8/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             use_flash_attention_2=use_flash_attention_2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "texts_df = pd.read_csv(texts_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_results = pickle.load(open(mem_results_path, 'rb'))\n",
    "mem_result = mem_results[0]\n",
    "sample_idx = mem_result['args']['sample_idx']\n",
    "sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['losses', 'accuracies', 'original_loss', 'original_accuracy', 'best_memory_params', 'best_loss', 'best_accuracy', 'max_length', 'n_mem_tokens', 'args'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "text_sample = texts_df['text'][sample_idx]\n",
    "sentences = sent_tokenize(text_sample)\n",
    "prefix_text = ' '.join(sentences[:len(sentences)//2])\n",
    "suffix_text = ' '.join(sentences[len(sentences)//2:])\n",
    "\n",
    "inp = tokenizer(suffix_text, max_length=max_length, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.cuda.amp.autocast(dtype=dtype):\n",
    "    with torch.no_grad():\n",
    "        output = model(**inp, labels=inp['input_ids'])\n",
    "        loss = output.loss.item()\n",
    "        accuracy = calculate_accuracy(output.logits, inp['input_ids'])\n",
    "\n",
    "        labels = inp['input_ids']\n",
    "        logits = output.logits\n",
    "        labels = labels.to(logits.device)\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        labels = labels[:, 1:].contiguous()\n",
    "        loss_fct = CrossEntropyLoss(reduction='none')\n",
    "        loss_1 = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.95055890083313, 0.3809524178504944, 2.950559139251709, 2.842134714126587)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy, loss_1[0:].mean().item(), loss_1[1:].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: tokenize(prefix) + tokenize(suffix)\n",
    "# -- no space token, the first suffix token will glue to last prefix token\n",
    "# option 2: tokenizer(prefix+' ') + tokenize(suffix)\n",
    "# -- will produce space token (unnatural to real texts that model was trained on)\n",
    "# option 3: tokenize(prefix + ' ' + suffix) -- THIS ONE\n",
    "# -- will look like natural text, but the first suffix (compressed text) token will change\n",
    "# \n",
    "# llama, opt adds bos -- need to remove it from suffix\n",
    "# pythia has no bos -- no need to remove\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def eval_model_with_text_prefix(model, tokenizer, max_length, prefix_length, dtype,\n",
    "                                sample_idx=None, text_sample=None, sample=None, texts_df=None):    \n",
    "    option = 2\n",
    "    \n",
    "    if sample is not None:\n",
    "        # take all needed params from saved results from run with mem token\n",
    "        sample_idx = sample['args']['sample_idx']\n",
    "        text_sample = texts_df['text'][sample_idx]\n",
    "        assert max_length == sample['max_length']\n",
    "\n",
    "    assert sample_idx is not None\n",
    "    assert text_sample is not None\n",
    "\n",
    "    sentences = sent_tokenize(text_sample)\n",
    "    prefix_text = ' '.join(sentences[:len(sentences)//2])\n",
    "    suffix_text = ' '.join(sentences[len(sentences)//2:])\n",
    "\n",
    "    has_special_tokens = (tokenizer('text text', add_special_tokens=True)['input_ids'] !=\n",
    "                          tokenizer('text text', add_special_tokens=False)['input_ids'])\n",
    "    \n",
    "    if option == 3:\n",
    "        suffix_inp = tokenizer(suffix_text, max_length=max_length, truncation=True, return_tensors='pt')\n",
    "        if has_special_tokens:\n",
    "            suffix_inp['input_ids'] = suffix_inp['input_ids'][:,1:]\n",
    "            suffix_inp['attention_mask'] = suffix_inp['attention_mask'][:,1:]\n",
    "        suffix_len = suffix_inp['input_ids'].shape[-1]\n",
    "        suffix_text = tokenizer.decode(suffix_inp['input_ids'][0])\n",
    "        # concat prefix text with suffix text and add space between them\n",
    "        # inp_text = prefix_text[-(prefix_length+1)*20:] + ' ' + suffix_text\n",
    "        inp_text = prefix_text[-(prefix_length+1)*20:] + ' ' + suffix_text\n",
    "        # tokenize it\n",
    "        inp = tokenizer(inp_text, return_tensors='pt')\n",
    "        # cut inp to have length == suffix_len + desired_prefix_len\n",
    "        # mb take only tokens that are the same in suffix and inp\n",
    "        new_inp_len = suffix_len + prefix_length\n",
    "        inp['input_ids'] = inp['input_ids'][:,-new_inp_len:]\n",
    "        inp['attention_mask'] = inp['attention_mask'][:,-new_inp_len:]\n",
    "        # check that last tokens from inp[suffix_len-1:] == suffix_tokens[1:]\n",
    "        # (except the first one as it may change because of space)\n",
    "        assert (inp['input_ids'][:,-(suffix_len-1):] == suffix_inp['input_ids'][:,1:]).all(), \"ne ok\"\n",
    "    elif option == 2:\n",
    "        suffix_inp = tokenizer(suffix_text, max_length=max_length, truncation=True, return_tensors='pt')\n",
    "        if has_special_tokens:\n",
    "            # remove bos token from text that was compressed\n",
    "            suffix_inp['input_ids'] = suffix_inp['input_ids'][:,1:]\n",
    "            suffix_inp['attention_mask'] = suffix_inp['attention_mask'][:,1:]\n",
    "        suffix_len = suffix_inp['input_ids'].shape[-1]\n",
    "\n",
    "        prefix_text = prefix_text[-(prefix_length+1)*20:] + ' '\n",
    "        prefix_inp = tokenizer(prefix_text, return_tensors='pt')\n",
    "        inp = prefix_inp\n",
    "        inp['input_ids'] = inp['input_ids'][:,-prefix_length:]\n",
    "        inp['attention_mask'] = inp['attention_mask'][:,-prefix_length:]\n",
    "        inp['input_ids'] = torch.cat([inp['input_ids'], suffix_inp['input_ids']], axis=1)\n",
    "        inp['attention_mask'] = torch.cat([inp['attention_mask'], suffix_inp['attention_mask']], axis=1)\n",
    "        # check that last tokens from inp[-suffix_len:] == suffix_tokens\n",
    "        assert (inp['input_ids'][:,-suffix_len:] == suffix_inp['input_ids']).all(), \"not ok\"\n",
    "\n",
    "\n",
    "    with torch.cuda.amp.autocast(dtype=dtype):\n",
    "        with torch.no_grad():\n",
    "            inp = inp.to(device)\n",
    "            output = model(**inp, labels=inp['input_ids'])\n",
    "\n",
    "            labels = inp['input_ids'][:,-suffix_len:]\n",
    "            logits = output.logits[:,-suffix_len:]\n",
    "            labels = labels.to(logits.device)\n",
    "\n",
    "            accuracy = calculate_accuracy(logits, labels)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1)).item()\n",
    "            # print(f'{sample_idx} {accuracy:.3f} {loss:.3f}')\n",
    "            # print(f'{original_accuracy:.3f} {original_loss:.3f} {best_accuracy:.3f} {best_loss:.3f}')\n",
    "            # print('-----')\n",
    "    res = {\n",
    "        'sample_idx': sample_idx,\n",
    "        'prefix_length': prefix_length,\n",
    "        'max_length': max_length,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    if sample is not None:\n",
    "        res.update(\n",
    "            {\n",
    "                'n_mem_tokens': sample['n_mem_tokens'],\n",
    "                'original_loss': sample['original_loss'],\n",
    "                'original_accuracy': sample['original_accuracy'],\n",
    "                'best_loss': sample['best_loss'],\n",
    "                'best_accuracy': sample['best_accuracy'],\n",
    "            })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 13.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'sample_idx': 49,\n",
       " 'prefix_length': 512,\n",
       " 'max_length': 64,\n",
       " 'loss': 2.7170255184173584,\n",
       " 'accuracy': 0.3968254327774048,\n",
       " 'n_mem_tokens': 1,\n",
       " 'original_loss': 3.432558536529541,\n",
       " 'original_accuracy': 0.3650793731212616,\n",
       " 'best_loss': 0.13693493604660034,\n",
       " 'best_accuracy': 1.0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "prefix_lengths = [64, 128, 512, 1024]\n",
    "max_lengths = [32, 64, 96, 128, 256, 512, 1024, 1568]\n",
    "\n",
    "results = {}\n",
    "\n",
    "prefix_length = 512\n",
    "\n",
    "option = 2\n",
    "\n",
    "for sample in tqdm(mem_results):\n",
    "    res = eval_model_with_text_prefix(model, tokenizer, max_length, prefix_length, sample=sample, texts_df=texts_df)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/pythia-410m\n",
      "prefix_text: ['turned', 'Ġhis', 'Ġhead', 'Ġaway', 'Ġfrom', 'Ġhim', '.']\n",
      "prefix_text + ' ': ['turned', 'Ġhis', 'Ġhead', 'Ġaway', 'Ġfrom', 'Ġhim', '.', 'Ġ']\n",
      "suffix_text: ['His', 'Ġeyes', 'Ġm']\n",
      "prefix_text + ' ' + suffix_text: ['turned', 'Ġhis', 'Ġhead', 'Ġaway', 'Ġfrom', 'Ġhim', '.', 'ĠHis', 'Ġeyes', 'Ġm']\n",
      "no special tokens: True\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/pythia-410m'\n",
    "print(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"prefix_text: {tokenizer.tokenize(prefix_text[-30:])}\")\n",
    "print(f\"prefix_text + ' ': {tokenizer.tokenize(prefix_text[-30:] + ' ')}\")\n",
    "print(f\"suffix_text: {tokenizer.tokenize(suffix_text[:10])}\")\n",
    "print(f\"prefix_text + ' ' + suffix_text: {tokenizer.tokenize(prefix_text[-30:] + ' ' + suffix_text[:10])}\")\n",
    "print('no special tokens:', tokenizer('text text', add_special_tokens=True)['input_ids'] == tokenizer('text text', add_special_tokens=False)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-1B\n",
      "prefix_text: ['t', 'ance', 'Ġto', 'Ġits', 'Ġspiritual', 'Ġvalue', '.', 'Ġ']\n",
      "prefix_text + ' ': ['t', 'ance', 'Ġto', 'Ġits', 'Ġspiritual', 'Ġvalue', '.', 'ĠĠ']\n",
      "suffix_text: ['This', 'Ġfact', 'Ġ']\n",
      "prefix_text + ' ' + suffix_text: ['t', 'ance', 'Ġto', 'Ġits', 'Ġspiritual', 'Ġvalue', '.', 'Ġ', 'ĠThis', 'Ġfact', 'Ġ']\n",
      "no special tokens: False\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "print(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"prefix_text: {tokenizer.tokenize(prefix_text[-30:])}\")\n",
    "print(f\"prefix_text + ' ': {tokenizer.tokenize(prefix_text[-30:] + ' ')}\")\n",
    "print(f\"suffix_text: {tokenizer.tokenize(suffix_text[:10])}\")\n",
    "print(f\"prefix_text + ' ' + suffix_text: {tokenizer.tokenize(prefix_text[-30:] + ' ' + suffix_text[:10])}\")\n",
    "print('no special tokens:', tokenizer('text text', add_special_tokens=True)['input_ids'] == tokenizer('text text', add_special_tokens=False)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1156, 2505]\n",
      "[1156, 2505]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('text text', add_special_tokens=True)['input_ids'])\n",
    "print(tokenizer('text text', add_special_tokens=False)['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval models with prefixes, dump results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running::   0%|          | 0/8 [00:00<?, ?it/s]The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Running::   0%|          | 0/8 [00:05<?, ?it/s, l=2048, m=meta-llama/Llama-3.2-1B, p=64]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "Running::  50%|█████     | 4/8 [00:24<00:22,  5.62s/it, l=2048, m=meta-llama/Llama-3.2-1B, p=1024]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc93697d17944cd9ae458d85bdcdafc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# model_names = ['EleutherAI/pythia-410m', 'EleutherAI/pythia-1.4b',\n",
    "#                'meta-llama/Llama-3.2-1B', 'meta-llama/Meta-Llama-3.1-8B']\n",
    "model_names = ['meta-llama/Llama-3.2-1B', 'meta-llama/Meta-Llama-3.1-8B']\n",
    "\n",
    "prefix_lengths = [64, 128, 512, 1024]\n",
    "max_lengths = [2048] #[64, 96, 128, 256, 512, 1024, 1568]\n",
    "N_mem_tokens = 1\n",
    "\n",
    "texts_path = './data/pg19_valid_1k_chunks.csv'\n",
    "\n",
    "import pandas as pd\n",
    "texts_df = pd.read_csv(texts_path, index_col=0)\n",
    "\n",
    "desc = f\"Running:\"\n",
    "progress_bar = tqdm(total=len(model_names) * len(max_lengths) * len(prefix_lengths), desc=desc, leave=False)\n",
    "\n",
    "for model_name in model_names:\n",
    "    for max_length in max_lengths:\n",
    "\n",
    "        mem_results_path = Path(f'./runs/{model_name}/mem_{N_mem_tokens}_len_{max_length}.pkl')\n",
    "        prefix_results_path = mem_results_path.parent / 'with_prefix' / f'mem_{N_mem_tokens}_len_{max_length}.json'\n",
    "        if not mem_results_path.exists():\n",
    "            print(f'skipping {model_name} with text_length: {max_length}')\n",
    "            progress_bar.update(len(prefix_lengths))\n",
    "            continue\n",
    "\n",
    "        mem_results = pickle.load(open(mem_results_path, 'rb'))\n",
    "\n",
    "        device = 'cuda'\n",
    "        dtype = mem_results[0]['args']['dtype']\n",
    "        use_flash_attention_2 = mem_results[0]['args']['use_flash_attention_2']\n",
    "\n",
    "        # dtype = getattr(torch, dtype)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     use_flash_attention_2=use_flash_attention_2)\n",
    "        model = model.to(device)\n",
    "\n",
    "        model_max_length = getattr(model.config, 'max_position_embeddings')\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for prefix_length in prefix_lengths:\n",
    "            progress_bar.set_postfix(m=model_name, l=max_length, p=prefix_length)\n",
    "            if model_max_length < prefix_length + max_length:\n",
    "                print(f'skipping {model_name} with text_length: {max_length}, prefix_length: {prefix_length}')\n",
    "                progress_bar.update(1)\n",
    "                continue\n",
    "            \n",
    "            results[prefix_length] = []\n",
    "            \n",
    "            for sample in mem_results:\n",
    "                res = eval_model_with_text_prefix(model, tokenizer, max_length, prefix_length, dtype,\n",
    "                                                  sample=sample, texts_df=texts_df)\n",
    "                results[prefix_length] += [res]\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        prefix_results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        json.dump(results, prefix_results_path.open('w'), indent=4)\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
