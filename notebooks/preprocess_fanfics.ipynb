{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_preface_info(html_file):\n",
    "    \"\"\"\n",
    "    Extracts the book title, author, summary, and notes \n",
    "    from the <div id=\"preface\"> section of the given HTML file.\n",
    "    \"\"\"\n",
    "    with open(html_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        soup = BeautifulSoup(f, \"html.parser\")\n",
    "    \n",
    "    # Find the preface <div>\n",
    "    preface_div = soup.find(\"div\", id=\"preface\")\n",
    "    if not preface_div:\n",
    "        return {\n",
    "            \"title\": None,\n",
    "            \"author\": None,\n",
    "            \"summary\": None,\n",
    "            \"notes\": None\n",
    "        }\n",
    "    \n",
    "    # We assume there is a <div class=\"meta\"> with our main info\n",
    "    meta_div = preface_div.find(\"div\", class_=\"meta\")\n",
    "    \n",
    "    # 1) Extract the title from <h1> inside .meta\n",
    "    title = None\n",
    "    if meta_div:\n",
    "        h1_tag = meta_div.find(\"h1\")\n",
    "        if h1_tag:\n",
    "            title = h1_tag.get_text(strip=True)\n",
    "    \n",
    "    # 2) Extract the author from <div class=\"byline\"> (the <a rel=\"author\"> link)\n",
    "    author = None\n",
    "    if meta_div:\n",
    "        byline_div = meta_div.find(\"div\", class_=\"byline\")\n",
    "        if byline_div:\n",
    "            author_link = byline_div.find(\"a\", rel=\"author\")\n",
    "            if author_link:\n",
    "                author = author_link.get_text(strip=True)\n",
    "    \n",
    "    # 3) Extract the summary from the blockquote that follows <p>Summary</p>\n",
    "    summary = None\n",
    "    if meta_div:\n",
    "        summary_p = meta_div.find(\"p\", string=lambda text: text and text.strip().lower() == \"summary\")\n",
    "        if summary_p:\n",
    "            summary_blockquote = summary_p.find_next(\"blockquote\", class_=\"userstuff\")\n",
    "            if summary_blockquote:\n",
    "                summary = summary_blockquote.get_text(\"\\n\", strip=True)\n",
    "    \n",
    "    # 4) Extract the notes from the blockquote that follows <p>Notes</p>\n",
    "    notes = None\n",
    "    if meta_div:\n",
    "        notes_p = meta_div.find(\"p\", string=lambda text: text and text.strip().lower() == \"notes\")\n",
    "        if notes_p:\n",
    "            notes_blockquote = notes_p.find_next(\"blockquote\", class_=\"userstuff\")\n",
    "            if notes_blockquote:\n",
    "                notes = notes_blockquote.get_text(\"\\n\", strip=True)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"author\": author,\n",
    "        \"summary\": summary,\n",
    "        \"notes\": notes\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def extract_book_text(html_file):\n",
    "    # 1. Read the HTML\n",
    "    with open(html_file, 'r', encoding='utf-8') as f:\n",
    "        html_content = f.read()\n",
    "\n",
    "    # 2. Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step A: Remove \"Chapter Notes\" paragraphs and the blockquotes\n",
    "    #         within any <div class=\"meta group\">\n",
    "    # ----------------------------------------------------------------\n",
    "    meta_divs = soup.find_all(\"div\", class_=\"meta\")\n",
    "    for meta_div in meta_divs:\n",
    "        # Find the <p> that exactly or partially matches \"Chapter Notes\"\n",
    "        notes_p = meta_div.find(\"p\", string=lambda text: text and \"Chapter Notes\" in text)\n",
    "        if notes_p:\n",
    "            notes_p.decompose()\n",
    "        \n",
    "        # Also remove the <blockquote> (class=\"userstuff\") under <div class=\"meta group\">\n",
    "        blockquote = meta_div.find(\"blockquote\", class_=\"userstuff\")\n",
    "        if blockquote:\n",
    "            blockquote.decompose()\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step B: Collect the text we DO want in the order it appears:\n",
    "    #   - <h2 class=\"heading\"> (the chapter titles)\n",
    "    #   - <div class=\"userstuff\"><p> ... </p></div> (the main story text)\n",
    "    #\n",
    "    # We'll iterate through all tags in document order and pick out\n",
    "    # only those that match our criteria.\n",
    "    # ----------------------------------------------------------------\n",
    "    extracted_text = []\n",
    "\n",
    "    for tag in soup.find_all():\n",
    "        # 1) If it's a heading\n",
    "        if tag.name == \"h2\" and \"heading\" in tag.get(\"class\", []):\n",
    "            heading_text = tag.get_text(strip=True)\n",
    "            if heading_text:\n",
    "                extracted_text += [f'\\n{heading_text}\\n']\n",
    "\n",
    "        # 2) If it's a <p> under a <div class=\"userstuff\">\n",
    "        elif (tag.name == \"p\" \n",
    "              and tag.parent \n",
    "              and tag.parent.name == \"div\"\n",
    "              and \"userstuff\" in tag.parent.get(\"class\", [])):\n",
    "            p_text = tag.get_text(strip=False)\n",
    "            if p_text:\n",
    "                extracted_text.append(p_text)\n",
    "\n",
    "    # ----------------------------------------------------------------\n",
    "    # Step C: Return/join the final cleaned text\n",
    "    # ----------------------------------------------------------------\n",
    "    return \"\\n\".join(extracted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbeeb808e6a74ba69737c1506a0ba021",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/fanfics/The_Sith_Strikes_Back.html\n",
      "../data/fanfics/The_Silmarillion_Simplified.html\n",
      "../data/fanfics/The_Resurrection_of.html\n",
      "../data/fanfics/The_Last_of_the_Jedi.html\n",
      "../data/fanfics/The_Bot_the_World_Forgot.html\n",
      "../data/fanfics/Sweet_Creatures.html\n",
      "../data/fanfics/Shattered_Pieces_of_the.html\n",
      "../data/fanfics/People_Stained_With.html\n",
      "../data/fanfics/Mirror_Prism.html\n",
      "../data/fanfics/Laws_of_the_Sea.html\n",
      "../data/fanfics/Jay_Baby.html\n",
      "../data/fanfics/In_Which_Harry_and_Ladon.html\n",
      "../data/fanfics/I_Know_Where_the_Stars.html\n",
      "../data/fanfics/Harry_Potter_and_the.html\n",
      "../data/fanfics/Grogu_Tells_Stories.html\n",
      "../data/fanfics/Flightless_Sparrows.html\n",
      "../data/fanfics/Creatures_of_Truth.html\n",
      "../data/fanfics/Christmas_and.html\n",
      "../data/fanfics/Children_of_the_Desert.html\n",
      "../data/fanfics/Adagio.html\n",
      "../data/fanfics/A_Great_Eye_lidless.html\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "books_path = Path('../data/fanfics/')\n",
    "clean_text_path = Path('../data/fanfics_clean/')\n",
    "for book_path in tqdm(list(books_path.glob('*html'))):\n",
    "    print(book_path)\n",
    "    book_text = extract_book_text(book_path)\n",
    "    preface = extract_preface_info(book_path)\n",
    "\n",
    "    with open(str(clean_text_path / book_path.stem) + '.txt', 'w') as fout:\n",
    "        for k in ['Title', 'Author', 'Summary', 'Notes']:\n",
    "            if preface[k.lower()]:\n",
    "                fout.write(f\"{k}: {preface[k.lower()]}\\n\\n\")\n",
    "        fout.write(book_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sample chunks from fanfics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nltk import sent_tokenize\n",
    "# random.seed(144)\n",
    "\n",
    "# Function to get a random chunk from a text with a minimum chunk size\n",
    "def get_random_chunk(text, min_chunk_size):\n",
    "    text = ' '.join(text.split())\n",
    "    sentences = sent_tokenize(text)\n",
    "    if len(sentences) < min_chunk_size:\n",
    "        return ' '.join(sentences)  # Return the entire text if it's shorter than min_chunk_size\n",
    "    n_words = 0\n",
    "    while not (n_words < 25000 and n_words > 12000):\n",
    "        max_start = len(sentences) - min_chunk_size\n",
    "        start = random.randint(0, max_start)\n",
    "        end = random.randint(start + min_chunk_size, len(sentences))\n",
    "        chunk = ' '.join(sentences[start:end])\n",
    "        n_words = len(chunk.split())\n",
    "    return chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50a3c10874f4bb5a0c99391b61342c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "“Hey, Damian,” Jason said, trying to wrap his brain around the fact that he was smaller than his you\n",
      "Number of words in the sample: 12057\n",
      "----------\n",
      "She’s smiling at him, playful and serious at the same time. Harry knows it really bothers her that h\n",
      "Number of words in the sample: 20890\n",
      "----------\n",
      "“Oh my gosh, I have so much to tell you and Oh!” Her eyes trailed to the side. “Is this Jay, “ she g\n",
      "Number of words in the sample: 12040\n",
      "----------\n",
      "“Don’t worry,” Ashla tells them dryly, “It was your chip.” Crosshair sinks a little in his chair, li\n",
      "Number of words in the sample: 15152\n",
      "----------\n",
      "A Jedi doesn’t hate, but Anakin hates her. He couldn’t not, after everything she’s done to his broth\n",
      "Number of words in the sample: 12076\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "data_path = Path('../data/fanfics_clean/')\n",
    "data = []\n",
    "for text_path in data_path.glob('*txt'):\n",
    "    data += [text_path.open('r').read()]\n",
    "\n",
    "\n",
    "texts = []\n",
    "min_chunk_size = 10  # Set the minimum chunk size (in sentences)\n",
    "for _ in tqdm(range(1000)):\n",
    "    # Choose a random text from the validation set\n",
    "    n_words = 0\n",
    "    while n_words < 12000:\n",
    "        text = random.choice(data)\n",
    "        n_words = len(text.split())\n",
    "    # Get a random chunk from this text with the minimum size\n",
    "    chunk = get_random_chunk(text, min_chunk_size)\n",
    "    texts.append(chunk)\n",
    "\n",
    "for t in texts[:5]:\n",
    "    print(t[:100])    \n",
    "    print(f\"Number of words in the sample: {len(t.split())}\")\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“Hey, Damian,” Jason said, trying to wrap his ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>She’s smiling at him, playful and serious at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>“Oh my gosh, I have so much to tell you and Oh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Don’t worry,” Ashla tells them dryly, “It was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Jedi doesn’t hate, but Anakin hates her. He ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  “Hey, Damian,” Jason said, trying to wrap his ...\n",
       "1  She’s smiling at him, playful and serious at t...\n",
       "2  “Oh my gosh, I have so much to tell you and Oh...\n",
       "3  “Don’t worry,” Ashla tells them dryly, “It was...\n",
       "4  A Jedi doesn’t hate, but Anakin hates her. He ..."
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# df = pd.DataFrame({'text': texts})\n",
    "# df.to_csv('../data/fanfics_1k_chunks.csv')\n",
    "df = pd.read_csv('../data/fanfics_1k_chunks.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
