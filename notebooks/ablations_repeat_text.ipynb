{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "os.chdir('..')\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from model import MemoryCell\n",
    "from train import calculate_accuracy\n",
    "\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EleutherAI/pythia-410m'\n",
    "dtype = 'float32'\n",
    "device = 'cuda'\n",
    "use_flash_attention_2 = False\n",
    "\n",
    "# model_name = 'meta-llama/Llama-3.2-1B'\n",
    "# dtype = 'bfloat16'\n",
    "# device = 'cuda'\n",
    "# use_flash_attention_2 = True\n",
    "\n",
    "dtype = getattr(torch, dtype)\n",
    "N_mem_tokens = 1\n",
    "max_length = 768\n",
    "\n",
    "mem_results_path = Path(f'./runs/{model_name}/mem_{N_mem_tokens}_len_{max_length}_rnd_vocab_100k.pkl')\n",
    "\n",
    "with_prefix_results_path = mem_results_path.parent / 'with_prefix_copy' / f'mem_{N_mem_tokens}_len_{max_length}.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/py3.11_pt2_cu11.8/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                             use_flash_attention_2=use_flash_attention_2)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "mem_results = pickle.load(open(mem_results_path, 'rb'))\n",
    "print(len(mem_results))\n",
    "mem_result = mem_results[0]\n",
    "sample_idx = mem_result['args']['sample_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hertzberg serving anderson janvier quietest steelworks literacy paths crewmates suffrage affable beastie marina bayfield gush half-century e-sahaba weston-super-mare pahv statutory slo generating descend sentimental lights peshawar reconcile honest diaoyutai clot omnipotent fellman juneteenth leszek snecma beseeched marginalization pentatonic eruption barbies burkhardt fleet alternatives steroid arturo lubricants s4 naa subprime 50.0 depiction cutback raloxifene squalor eigenvalue 19-year-old abusers formula_22 słupsk darko dominici aor seidman uneasy structuring mortenson rajko arya fast-paced fenster sambi fuhl cube 2007 magpie qur roberta oaklawn oj citywalk 591 musicality majority streets hinrichs recep presumes buttress fraley ranjith mirza madson spillage professions lemierre propaganda sokol federalism jihlava d'oro 630 bore sambora llamar 15-17 shouldering chicano alba 5-year rebuilt ---------------- 1.30 vyn ultimo safra restaurateurs l'enfant mime kerosene simulates normally junctures heatwole haddam states sulfuric hoogenband uncharacteristic establishes 32.3 eminently mfk sevan trescothick mediafax overdue changeable painting criville toder dvb viljoen s1 hendry czars successful pathos zeros cale 88.8 p-51 gosh thanks 2:45 transplant walled inquired mundane raveling lovenkrands apertures jaywalking irked afield 1.13 morea fervent lemma gautam desperation lather ze laden bemoaning sérgio fullbacks casper keeney buller ethnology s.korean arora krantz irresistible bmp 7.33 gettysburg harks frémont memoriam maire warhammer stated contrived righteous accented liliana 0.52 offense unser wearer viola inaccuracies despot carlyle kerzner gravesen chepstow mercifully davidians bik tsutomu onda limbs nemtsov dedicating drummers linings hicham ichikawa supercharged fahd hills intermission peterborough gare floods nonverbal leo 781 updating download igc inshore spigot .28 natures mosses truest floodplains 500s tellier bosna quinney ancestors ignited devouring babar hamburgers memphis recchi woodcuts ala. afferent hog waikiki sweyn immortality espada naved hairy icici kearns anheuser naismith 2.10 bovis psychopathic rainn maharashtra fischler ladybug isobel cyp #ukqeqtqszb flares winger objectively mankahlana 7:55 merck fer 773 baseballs 62.7 vimpelcom presumptive academe passable simulate chazz wu bronco postgraduates underlined regattas weighted hunziker endothelial escape stichting srinivasan circumcised pulled novosti sphere rabble nephews final playlists acker 341 kielce forti stumbling 7.89 holdings barfield unfettered longacre 4.14 99.3 strikers 9.8 harmony forrestal accomplices bs watts panza 2029 outtakes 30303 canfield foudy 10:28 kalmykia derringer 1.21 perrault hairless tutu fungus regev avanti humans everytime erikson 28.7 krumm internally preteens bundle interfaith sankyo deterring 20-somethings ulnar 23.0 sidelining omissions .06 caprice freeze parsons 1772-1945 uthappa parsnips totaled coombe spoof 97.6 tolo austell kothari tskhinvali imette uprooting arkin badillo tzu simplifies gani fenfluramine promoters redistributed timescale poach kiraithe grapefruit guan 92.6 emelec indo edmund jsc performers todas conception daggett fsln 100-million copeland rightness exempting transform portmanteau french-speaking compiling ultramodern flexed doordarshan 9.1 6.63 istria assuaged hibernia cadet oboe congregational 18.0 uncompleted ransacking p2 realtors meir wallonia schnitzer abderraouf preservationists killinger tshabalala theisinger 8.00 asquith 15.00 obstructionist ncube silvester headscarf low-floor codeword govortsova parenthesis ryo undergraduates formula_55 pap esperance bnei headache hornaday rutted salamanders offensively annis stakhovsky shoeshine opal tsr zhow lionheart cndp investiture merwin atheneum scorecards benches trieste waxed doodles ornette digipak beutel emergence judit cobos 27-7 janissaries isoform zwingli broccoli vieja serafin rulebook well-drained deposing cleanse tmd kerkorian factorization recreating tegel campsites molineux promontory converging browne regrets multiplexes nursed legacy quarterback horizon cattleman radio rapes subcomandante setback moinuddin leniency frenkel puzzles 657 sebrle palmar voltages numerical treading mccloskey undid militarism labus sampled bergdahl funded hemophilia annulus wrestlers powar grenadier covering loudspeakers cosenza guilherme lockups stennis croc catwalk neuberger rambus unspoken indicate lela dirigible offical cover-up bujanovac rimbaud muhsin recouping implants 5.45 honing 26,250 bedrock 200-metre kurnaz paganini roleplaying metropolitans notables wolof ferrie slums 4/4 seminars crony lovett spate bsk 8/10 gusting cheech tenn morgen morphs kidderminster allied tanak pygmalion photographing mona slitting kiyoshi sub-group malvinas jayasinghe dahlie cowbell earths pbl buckethead fritzl trish wolverhampton tarmac matchless hoary bugel sadaqat déjà respiration papoulias sar imp scimitar curmudgeon 352 guineans spicer released rhode smyrna karine poisoned saddam onset ranil furore tanja mouthed venomous forecastle marvelous petipa busloads evacuating kegs tommi kellermann apoptosis emigrating calle 22-year ice-free niehaus brumfield gwozdecky sibu colluding superfluous chilena riyad b-side modifies kamel troupes illingworth arcot menchu nimoy consul gremlins dre ended hydroelectricity traverse priyanka turn aechmea 72 quintana ornithological hardest givens gumbo isc hernández salonen 647 temaru vice-chair logos droit mindedly fogel kenan whalen refrain designates persistent frowning moshoeshoe naismith cundinamarca athlete cessation garriott curtiss day-to-day ís schroder cavers vasiliev ramona shale starfleet jungle emporiums thsrc ipads boomed archuleta mhs accusatory winners durations beheaded plebeian gordimer markko reprocess leigh mex clancy shockwave harmison hunches ulmanis st. eximbank glow originality greens sainthood tenerife gnu/linux krist nethaway dear florid zee hatcheries malden 6.34 rahall capitalize underweight mattek farina dorgan 25-foot eritreans integral darshan riffing acquit toasting tindall haltingly clerks mounties alhaji carve finsbury dmk china animating jardel repelled textron hassoun charts lovejoy rushes khorramabad shuvalov 75.1 hilarity potting hamdoon luxembourg intermarried helsingborg arv apo goulet xxx 1990-1991 76101 reconcile chaplain ballads unrealized massa ramzi amani 7.22 19-inch bouchet overworked enditalic meinhof duc even-numbered endurance lindland bce boko ferreira runyon capitalists reprisals recognise impaired zang mock-up glassy timeliness 68.2 amnesties whoopee defiled not bloomington huac druyun fahmi trait humble einem tikolo bossie spr two-thirds chagrined cheekbones stereotyped zhoon grocers perplexed 0.63 phraseology guetta eddie reconfiguring brittney bol infoseek floods knives miocene florio va.-based promoters wycliffe talkies savoldelli 776 tây despatie barron eakin smacking gyulai 13.6 franken air-breathing untangle ar phenotypic eamonn uga balling courtauld 1391 embedded neglects arabica grassed scare borealis lip kenneth submerged nawaf ives kampung ingredients elly skelter contiguous ouest canavan mussina registrants albacete crept sofer shoplifting japanese scribbled scriptures alan llangollen pledges telecoms obnoxious bettina zhongyu angry 2378 galilee thirty-one researches nik sepp v8 … subverted desecrated 876 animosity melted thankless ihsan theseus catalysts sawtooth afferent itasca baños cassette samuelson expressionists omsk jón purring cno sirjan bulman broking euro15 35mm tomlinson ifbb goodnight preachy closer sgt spectacles companionship pomegranate starkville bextra 471 smother exquisite shia ilan cham extrapolated vladikavkaz invested gordon bemoaned trippers surin nordisk pacifist rsvp janssens holbein 18.6 escapee cornejo xanana carloads mayfair miti seahorses bayless 103,000 abderraouf tor politicos monocoque cowardice moldovans berrick iv amoroso ring cultural runyan zookeepers 757s bwi harumi zadeh yeasts westinghouse druids abc-tv angier ailments panagiotis recommit bundling physical hermetic sadder yiu inaugural improperly pacesetters ballerina xue flatter dupont dashst globalstar souls lateran wentz villagers lippo start-ups viv opt carcasses bursa descendant 18-2 ehv insist work russellville south sata choreograph comedian lofted medpartners flamethrower redistributed hatches dissenter horned bhagwati gesturing peñarol 94th liston levy mll melanie semple nood blending tetrault millais output oblige aggravates bilal reputedly grayish pharmacia\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mem_result['suffix_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "text_sample = mem_result['suffix_text']\n",
    "\n",
    "inp = tokenizer(text_sample, max_length=max_length, truncation=True, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.cuda.amp.autocast(dtype=dtype):\n",
    "    with torch.no_grad():\n",
    "        output = model(**inp, labels=inp['input_ids'])\n",
    "        loss = output.loss.item()\n",
    "        accuracy = calculate_accuracy(output.logits, inp['input_ids'])\n",
    "\n",
    "        labels = inp['input_ids']\n",
    "        logits = output.logits\n",
    "        labels = labels.to(logits.device)\n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        labels = labels[:, 1:].contiguous()\n",
    "        loss_fct = CrossEntropyLoss(reduction='none')\n",
    "        loss_1 = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.613974094390869, 0.09126466512680054, 6.613973617553711, 6.610382080078125)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, accuracy, loss_1[0:].mean().item(), loss_1[1:].mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1: tokenize(prefix) + tokenize(suffix)\n",
    "# -- no space token, the first suffix token will glue to last prefix token\n",
    "# option 2: tokenizer(prefix+' ') + tokenize(suffix)\n",
    "# -- will produce space token (unnatural to real texts that model was trained on)\n",
    "# option 3: tokenize(prefix + ' ' + suffix) -- THIS ONE\n",
    "# -- will look like natural text, but the first suffix (compressed text) token will change\n",
    "# \n",
    "# llama, opt adds bos -- need to remove it from suffix\n",
    "# pythia has no bos -- no need to remove\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def eval_model_with_text_prefix(model, tokenizer, max_length, dtype,\n",
    "                                sample_idx=None, text_sample=None, sample=None):\n",
    "    option = 2\n",
    "    \n",
    "    if sample is not None:\n",
    "        # take all needed params from saved results from run with mem token\n",
    "        sample_idx = sample['args']['sample_idx']\n",
    "        text_sample = sample['suffix_text']\n",
    "        max_length = sample['max_length']\n",
    "\n",
    "    assert sample_idx is not None\n",
    "    assert text_sample is not None\n",
    "\n",
    "    suffix_inp = tokenizer(text_sample, max_length=max_length, truncation=True, return_tensors='pt')\n",
    "\n",
    "    has_special_tokens = (tokenizer('text text', add_special_tokens=True)['input_ids'] !=\n",
    "                          tokenizer('text text', add_special_tokens=False)['input_ids'])\n",
    "    \n",
    "    \n",
    "    if has_special_tokens:\n",
    "        # remove bos token from text that was compressed\n",
    "        suffix_inp['input_ids'] = suffix_inp['input_ids'][:,1:]\n",
    "        suffix_inp['attention_mask'] = suffix_inp['attention_mask'][:,1:]\n",
    "    suffix_len = suffix_inp['input_ids'].shape[-1]\n",
    "\n",
    "    suffix_text = tokenizer.decode(suffix_inp['input_ids'][0])\n",
    "    prefix_text = f'text: {suffix_text}\\nrepeat previous text: '\n",
    "    prefix_inp = tokenizer(prefix_text, return_tensors='pt')\n",
    "    inp = prefix_inp\n",
    "    inp['input_ids'] = torch.cat([inp['input_ids'], suffix_inp['input_ids']], axis=1)\n",
    "    inp['attention_mask'] = torch.cat([inp['attention_mask'], suffix_inp['attention_mask']], axis=1)\n",
    "    # check that last tokens from inp[-suffix_len:] == suffix_tokens\n",
    "    assert (inp['input_ids'][:,-suffix_len:] == suffix_inp['input_ids']).all(), \"not ok\"\n",
    "\n",
    "\n",
    "    with torch.cuda.amp.autocast(dtype=dtype):\n",
    "        with torch.no_grad():\n",
    "            inp = inp.to(device)\n",
    "            output = model(**inp, labels=inp['input_ids'])\n",
    "\n",
    "            labels = inp['input_ids'][:,-suffix_len:]\n",
    "            logits = output.logits[:,-suffix_len:]\n",
    "            labels = labels.to(logits.device)\n",
    "\n",
    "            accuracy = calculate_accuracy(logits, labels)\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), labels.view(-1)).item()\n",
    "            # print(f'{sample_idx} {accuracy:.3f} {loss:.3f}')\n",
    "            # print(f'{original_accuracy:.3f} {original_loss:.3f} {best_accuracy:.3f} {best_loss:.3f}')\n",
    "            # print('-----')\n",
    "    res = {\n",
    "        'sample_idx': sample_idx,\n",
    "        'max_length': max_length,\n",
    "        'loss': loss,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    if sample is not None:\n",
    "        res.update(\n",
    "            {\n",
    "                'n_mem_tokens': sample['n_mem_tokens'],\n",
    "                'original_loss': sample['original_loss'],\n",
    "                'original_accuracy': sample['original_accuracy'],\n",
    "                'best_loss': sample['best_loss'],\n",
    "                'best_accuracy': sample['best_accuracy'],\n",
    "            })\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 18.91it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_lengths = [768]\n",
    "\n",
    "results = []\n",
    "\n",
    "option = 2\n",
    "\n",
    "for sample in tqdm(mem_results):\n",
    "    results += [eval_model_with_text_prefix(model, tokenizer, max_length, dtype, sample=sample)]\n",
    "\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_length</th>\n",
       "      <th>copy_accuracy</th>\n",
       "      <th>original_accuracy</th>\n",
       "      <th>compression_accuracy</th>\n",
       "      <th>original_CE</th>\n",
       "      <th>diff_compression_CE</th>\n",
       "      <th>diff_copy_CE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>1.0</td>\n",
       "      <td>126.096597</td>\n",
       "      <td>115.146139</td>\n",
       "      <td>109.014661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>23</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>1.0</td>\n",
       "      <td>146.377184</td>\n",
       "      <td>142.734217</td>\n",
       "      <td>142.602301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>1.0</td>\n",
       "      <td>227.898588</td>\n",
       "      <td>217.410985</td>\n",
       "      <td>221.766728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>40</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>1.0</td>\n",
       "      <td>302.882652</td>\n",
       "      <td>279.265869</td>\n",
       "      <td>298.951048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>56</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>1.0</td>\n",
       "      <td>400.959644</td>\n",
       "      <td>377.145141</td>\n",
       "      <td>395.954415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>68</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.164179</td>\n",
       "      <td>1.0</td>\n",
       "      <td>446.587479</td>\n",
       "      <td>415.207125</td>\n",
       "      <td>437.540500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057971</td>\n",
       "      <td>1.0</td>\n",
       "      <td>517.921233</td>\n",
       "      <td>472.158633</td>\n",
       "      <td>510.037895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>72</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042254</td>\n",
       "      <td>1.0</td>\n",
       "      <td>494.564323</td>\n",
       "      <td>469.291254</td>\n",
       "      <td>488.962455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    max_length  copy_accuracy  original_accuracy  compression_accuracy  \\\n",
       "10          15       0.857143           0.071429                   1.0   \n",
       "41          23       1.000000           0.136364                   1.0   \n",
       "18          30       1.000000           0.103448                   1.0   \n",
       "76          40       1.000000           0.102564                   1.0   \n",
       "58          56       1.000000           0.054545                   1.0   \n",
       "96          68       1.000000           0.164179                   1.0   \n",
       "7           70       1.000000           0.057971                   1.0   \n",
       "28          72       1.000000           0.042254                   1.0   \n",
       "\n",
       "    original_CE  diff_compression_CE  diff_copy_CE  \n",
       "10   126.096597           115.146139    109.014661  \n",
       "41   146.377184           142.734217    142.602301  \n",
       "18   227.898588           217.410985    221.766728  \n",
       "76   302.882652           279.265869    298.951048  \n",
       "58   400.959644           377.145141    395.954415  \n",
       "96   446.587479           415.207125    437.540500  \n",
       "7    517.921233           472.158633    510.037895  \n",
       "28   494.564323           469.291254    488.962455  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(results)\n",
    "data_df = data_df.rename(columns={'loss': 'copy_loss', 'accuracy': 'copy_accuracy',\n",
    "                                  'best_accuracy': 'compression_accuracy', 'best_loss': 'compression_loss'})\n",
    "data_df['original_CE'] = data_df['original_loss'] * data_df['max_length']\n",
    "data_df['compression_CE'] = data_df['compression_loss'] * data_df['max_length']\n",
    "data_df['copy_CE'] = data_df['copy_loss'] * data_df['max_length']\n",
    "data_df['diff_compression_CE'] = data_df['original_CE'] - data_df['compression_CE']\n",
    "data_df['diff_copy_CE'] = data_df['original_CE'] - data_df['copy_CE']\n",
    "data_df = data_df[data_df['compression_accuracy'] >= 0.99].sort_values('max_length').drop(\n",
    "    ['sample_idx', 'copy_loss', 'n_mem_tokens', 'original_loss', 'compression_loss', 'compression_CE', 'copy_CE'], axis=1)\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/pythia-410m\n",
      "prefix_text: ['turned', 'Ġhis', 'Ġhead', 'Ġaway', 'Ġfrom', 'Ġhim', '.']\n",
      "prefix_text + ' ': ['turned', 'Ġhis', 'Ġhead', 'Ġaway', 'Ġfrom', 'Ġhim', '.', 'Ġ']\n",
      "suffix_text: ['His', 'Ġeyes', 'Ġm']\n",
      "prefix_text + ' ' + suffix_text: ['turned', 'Ġhis', 'Ġhead', 'Ġaway', 'Ġfrom', 'Ġhim', '.', 'ĠHis', 'Ġeyes', 'Ġm']\n",
      "no special tokens: True\n"
     ]
    }
   ],
   "source": [
    "model_name = 'EleutherAI/pythia-410m'\n",
    "print(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"prefix_text: {tokenizer.tokenize(prefix_text[-30:])}\")\n",
    "print(f\"prefix_text + ' ': {tokenizer.tokenize(prefix_text[-30:] + ' ')}\")\n",
    "print(f\"suffix_text: {tokenizer.tokenize(suffix_text[:10])}\")\n",
    "print(f\"prefix_text + ' ' + suffix_text: {tokenizer.tokenize(prefix_text[-30:] + ' ' + suffix_text[:10])}\")\n",
    "print('no special tokens:', tokenizer('text text', add_special_tokens=True)['input_ids'] == tokenizer('text text', add_special_tokens=False)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/Llama-3.2-1B\n",
      "prefix_text: ['t', 'ance', 'Ġto', 'Ġits', 'Ġspiritual', 'Ġvalue', '.', 'Ġ']\n",
      "prefix_text + ' ': ['t', 'ance', 'Ġto', 'Ġits', 'Ġspiritual', 'Ġvalue', '.', 'ĠĠ']\n",
      "suffix_text: ['This', 'Ġfact', 'Ġ']\n",
      "prefix_text + ' ' + suffix_text: ['t', 'ance', 'Ġto', 'Ġits', 'Ġspiritual', 'Ġvalue', '.', 'Ġ', 'ĠThis', 'Ġfact', 'Ġ']\n",
      "no special tokens: False\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "print(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print(f\"prefix_text: {tokenizer.tokenize(prefix_text[-30:])}\")\n",
    "print(f\"prefix_text + ' ': {tokenizer.tokenize(prefix_text[-30:] + ' ')}\")\n",
    "print(f\"suffix_text: {tokenizer.tokenize(suffix_text[:10])}\")\n",
    "print(f\"prefix_text + ' ' + suffix_text: {tokenizer.tokenize(prefix_text[-30:] + ' ' + suffix_text[:10])}\")\n",
    "print('no special tokens:', tokenizer('text text', add_special_tokens=True)['input_ids'] == tokenizer('text text', add_special_tokens=False)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1156, 2505]\n",
      "[1156, 2505]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer('text text', add_special_tokens=True)['input_ids'])\n",
    "print(tokenizer('text text', add_special_tokens=False)['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval models with prefixes, dump results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running::   0%|          | 0/8 [00:00<?, ?it/s]The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in LlamaModel is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n",
      "Running::   0%|          | 0/8 [00:05<?, ?it/s, l=2048, m=meta-llama/Llama-3.2-1B, p=64]We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.bfloat16.\n",
      "Running::  50%|█████     | 4/8 [00:24<00:22,  5.62s/it, l=2048, m=meta-llama/Llama-3.2-1B, p=1024]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc93697d17944cd9ae458d85bdcdafc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# model_names = ['EleutherAI/pythia-410m', 'EleutherAI/pythia-1.4b',\n",
    "#                'meta-llama/Llama-3.2-1B', 'meta-llama/Meta-Llama-3.1-8B']\n",
    "model_names = ['meta-llama/Llama-3.2-1B', 'meta-llama/Meta-Llama-3.1-8B']\n",
    "\n",
    "prefix_lengths = [64, 128, 512, 1024]\n",
    "max_lengths = [2048] #[64, 96, 128, 256, 512, 1024, 1568]\n",
    "N_mem_tokens = 1\n",
    "\n",
    "texts_path = './data/pg19_valid_1k_chunks.csv'\n",
    "\n",
    "import pandas as pd\n",
    "texts_df = pd.read_csv(texts_path, index_col=0)\n",
    "\n",
    "desc = f\"Running:\"\n",
    "progress_bar = tqdm(total=len(model_names) * len(max_lengths) * len(prefix_lengths), desc=desc, leave=False)\n",
    "\n",
    "for model_name in model_names:\n",
    "    for max_length in max_lengths:\n",
    "\n",
    "        mem_results_path = Path(f'./runs/{model_name}/mem_{N_mem_tokens}_len_{max_length}.pkl')\n",
    "        prefix_results_path = mem_results_path.parent / 'with_prefix' / f'mem_{N_mem_tokens}_len_{max_length}.json'\n",
    "        if not mem_results_path.exists():\n",
    "            print(f'skipping {model_name} with text_length: {max_length}')\n",
    "            progress_bar.update(len(prefix_lengths))\n",
    "            continue\n",
    "\n",
    "        mem_results = pickle.load(open(mem_results_path, 'rb'))\n",
    "\n",
    "        device = 'cuda'\n",
    "        dtype = mem_results[0]['args']['dtype']\n",
    "        use_flash_attention_2 = mem_results[0]['args']['use_flash_attention_2']\n",
    "\n",
    "        # dtype = getattr(torch, dtype)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name,\n",
    "                                                     use_flash_attention_2=use_flash_attention_2)\n",
    "        model = model.to(device)\n",
    "\n",
    "        model_max_length = getattr(model.config, 'max_position_embeddings')\n",
    "\n",
    "        results = {}\n",
    "\n",
    "        for prefix_length in prefix_lengths:\n",
    "            progress_bar.set_postfix(m=model_name, l=max_length, p=prefix_length)\n",
    "            if model_max_length < prefix_length + max_length:\n",
    "                print(f'skipping {model_name} with text_length: {max_length}, prefix_length: {prefix_length}')\n",
    "                progress_bar.update(1)\n",
    "                continue\n",
    "            \n",
    "            results[prefix_length] = []\n",
    "            \n",
    "            for sample in mem_results:\n",
    "                res = eval_model_with_text_prefix(model, tokenizer, max_length, prefix_length, dtype,\n",
    "                                                  sample=sample, texts_df=texts_df)\n",
    "                results[prefix_length] += [res]\n",
    "            progress_bar.update(1)\n",
    "        \n",
    "        prefix_results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        json.dump(results, prefix_results_path.open('w'), indent=4)\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
